# Software preservation is necessary for reproducibility
**by Vicky Rampin**  

## From session 3 - [Human factors and new development in preserving and sustaining research software](/wosss21/agenda#session-3)  
As supporting research reproducibility continues to take shape throughout various scholarly communities, we’ve seen a number of tools arise to help. However, most if not all of the current tools for reproducibility were made for short-term replay of research, relying on container technology and having researchers manually configure their computational environments. This is problematic for long-term access to research in many ways, particularly because it’s often incredibly difficult (neigh on impossible) to uncover all the dependencies that computational research touches, and even harder to make sure those persist in the long-term (and we know that the computational environment where research takes place directly affects it’s analytical result -- see this one example from [Ars Technica](https://arstechnica.com/information-technology/2019/10/chemists-discover-cross-platform-python-scripts-not-so-cross-platform/)). The more that we care about reproducibility, the more it becomes clear that we rely on software preservation in a number of ways to enable that reproducibility.

In this talk, I’ll go over [ReproZip](https://www.reprozip.org/), which adds automation,  extensibility, and preservation of reproducible research to the current landscape of tools. ReproZip has been in development at New York University since 2013, and ReproZip bundles that were created back then are still fully rerunnable and reproducible today, which few other tools can boast (if any).

ReproZip helps researchers make long-term reproducible bundles of their work in two steps:
1. the tracing step: researchers run ReproZip at the same time they run some analyses, program, pipeline, etc., and ReproZip detects and captures the source of everything that the process touches (input data, source code, environment variables -- everything!).
2. the packing step: researchers bundle all that information in a .rpz file. The bundle is generalizable enough to be able to be viewed and reused by a number of other tools (which improves the sustainability of ReproZip bundles).

ReproZip bundles can then be used to automatically set up the original researchers’ computational environment and project workflow on someone else’s computer, which is ideal for computational reproducibility. Secondary users can verify the original researcher's work, but also extend it by using their own input data. Because ReproZip packs all the source of everydependency of a script, workflow, etc., as well as the provenance information (for instance, the order in which the original researcher runs multiple scripts), it allows for high fidelity and long-term reproducibility at low cost to the user (in terms of time, resources, and labor).

Among the many benefits of ReproZip, sustainability and preservation is the most unique in the landscape of existing tools. The rpz file generated by ReproZip is preservation-ready from the time it is created, for a few main reasons:

 - **Flexibility**: the rpz file is completely agnostic to the unpacking technology being used, and this has allowed ReproZip to be leveraged in many preservation contexts. ReproZip itself also uses a plugin model such that unpackers can be added and removed as the march of time goes on. This has already resulted in ReproZip being used widely for many use cases.
 - **Completeness**: the rpz file contains all the necessary files to reproduce the packed research, as well as a highly detailed metadata file (config.yml) that lists all the technical and administrative metadata about the computational environment being used, the workflow steps, and other dependencies packed. If for some reason there are no containers or virtual machines in the future, then someone could use this metadata file to rebuild the computational environment, and use the files in the bundle itself to reproduce the work.

ReproZip is already integrated in a number of systems and workflows to support many different use cases, again adding in valuable automation, sustainability, and preservation capabilities. Some of these include:
 - Computational science tools ([NeuroDocker](https://github.com/ReproNim/neurodocker) to minify docker containers, [Spot](https://pubmed.ncbi.nlm.nih.gov/33269388/) to reconstruct provenance graphs, [Model Insertion Checker](https://mic-cli.readthedocs.io/en/latest/model_configuration/01-overview/#step-2-trace-your-model-execution) (part of DARPA’s World Modelers program) to trace and pack model execution)
 - Peer review (ex: [SIGMOD](https://reproducibility.sigmod.org/#process))
 - Querying metadata at scale (ex: [WholeTale](https://reusableresearch.com/slides/bludaescher.pdf), [explore web archives](https://twitter.com/anjacks0n/status/1323719989313048579))
 - Reproducibility component of other platforms ([Cloud of Reproducible Records](https://github.com/usnistgov/corr-reprozip))
 - Digital preservation infrastructure (ex: [Emulation as a Service Infrastructure](https://twitter.com/euanc/status/1143966909421019136), [ReproZip-Web](https://reprozip-web.readthedocs.io/en/latest/))

ReproZip clearly adds value in long-term access to reproducible research, as well as providing a pathway for the preservation of boutique research software and environments.
