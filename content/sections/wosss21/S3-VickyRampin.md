# Software preservation is necessary for reproducibility
**by Vicky Rampin**  

## From session 3 - [Human factors and new development in preserving and sustaining research software](/wosss21/agenda#session-3)  

The definitive write-up of this talk appears in the [WoSSS21 report](https://wosss.org/#reports).

### Resources

A [video](https://www.youtube.com/watch?v=h_FvXTNviY0&list=PLXAvKzjdTsrxFqbjWtxHjfJc0RN6jMwZg&index=18) and [slides](https://docs.google.com/presentation/d/16PIgUSRzOd_l1sBd62nHiXish8jFtfoyhDTvbH3uQlc/edit) are available.

### Transcript

#### Keywords

reproducibility, pack, software, computational, reprozip, research, sandbox environment, rerun, analytical results, tools, sustainability, sustainability efforts, data, retros, specific, dependency, rpz, scripts, auditable, environment

#### Text

Hi, my name is Vicky Rampin, I'm the librarian for research, data management and reproducibility at New York University. And I'm here to talk about why I think software preservation is necessary for reproducibility. And I'm going to go into a tool that I work on that tries to do some sustainability interventions at a few different place a few different places. So just to do some basic definition, setting reproducibility, when independent people use the same research materials and conditions to verify a claim. So this is like when a reviewer is rerunning someone's code during a peer review process to double check specific findings reported, you know, match what actually the outcome of the process is. So this doesn't actually necessarily test like the capital T truth of a claim. It's just like, you know, duh, did you do what you say you did? Yes. In order to test for reproducibility. However, because we're doing computational research, no matter if you're on an HPC, or running some SAS script on a Mac, the access to the computational environment, as well as the research materials are necessary in order to test reproducibility. So sustaining research software in service of reproducibility requires in depth curation of data and software together. But it's more about reproducing computational workflows in the long term. So this is a little different from other software sustainability efforts where it's like, you know, providing a sandbox environment where you can rerun a binary, right? This is about taking some data and code together and getting to a specific result. And it just so happens that it really it relies on some of that underlying computational environment to get at the right analytical result. I also really like this idea from starting at all, I served 2013 report that defines reproducibility in a more robust way. So it goes from reviewable, which is like what we have now you can read the Methods section, great, all the way down to auditable, which is the process and the tools are archived and reusable, such that you can again, test a claim that somebody is making test an analytical claim. And open and reproducible, of course, is the goal. It's where that auditable research is made openly available. And I like to actually think about it in terms of a pyramid, where you think like the article is the tip of the iceberg, the code in the data is necessary for you to replicate documentation is even more necessary even because, you know, give someone a spreadsheet and a Python script. And that's great, give someone some documentation about what the column headers are, and that research automatically becomes more usable. But underlying it all, again, is this idea that the actual computational environment where research is happening, matters to get at the same analytical results. So there are a few different entry points for software sustainability efforts, thinking about how it would apply in an academic context for reproducibility for letting people again, test claims in the long term. So I think yesterday, we heard from Euan and folks at Eaasi, and I would say like they're the big bottom of the pyramid, they're gonna be the sandbox environment. Like what I don't have access to Windows seven anymore. I know I can go to Eassi. So I love that, like they've got that bottom of the pyramid covered. Where I put in some of my effort is this business code and data part. So I think, in thinking about sustainability for research in higher ed, again, these are the two main entry points for adding in software sustainability, and they require different skill sets, and they require different tools, as well. And just some evidence for why the computational environment matters for reproducibility. This was a great story from Ars Technica, there are a million stories like this, it was about Python scripts used for analysis and chemistry. They because the scripts use this specific library called glob which returns a different sorted order, depending on the OS. They found that the scripts returned the correct results on Mac OS mavericks and Windows 10. But on Mac OS, Mojave and Ubuntu, the actual analytical results were off by nearly a full percent because of this.
 
There are around 160 papers that the author's identified as being affected. And it's really a problem of dependency, hell, right. And I added all the links to this in the notes, but dependency hell is the idea that like, your environment relies on something that relies on something etc, nesting super deeply, and you as a user might not even be aware of it. And I have many stories from my time as a reproducibility librarian dealing with dependency hell. Students, faculty, staff, everybody has a problem with this. And it's not just related to one technology like Python or one area like chemistry, we see it in lots of places. So here's just another quick example, where there are specific differences in neuroscience analyses depending on not only the operating system, but the version of the specific software and actually the hardware, the workstation type. And you can see on the graph here, it's, I can see it, well, hopefully, you can, too. But the results are actually off the analytical results. So I work on this tool called reprozip, reprozip was is an open source tool originally developed in 2013. at New York University where I work and reprozip does some interventions for software sustainability a few ways, but mainly, it captures and packs all of those dependencies. So getting at that problem of dependency, hell at sort of, again, that code and data level, and it automatically packs them into a shareable and preservation ready bundle the RPZ file, anybody can then take this RPC file and run the contents in the original research environment on any other different machine. So what it does is it you let's say, you pack something on Linux, and you want to rerun it on Mac reprozip, it will help you pack it on Linux, and then it will automatically set up the same original computational environment in on your Mac, to be able to rerun and get that same analytical result. Represent is nice for sustainability in a few ways, I'm going to go into this, but everything that our research process touches, so if you run a script that takes some data and get some output reprozip will know, it will not only pack like specific versions of libraries used, but specific interpreters. It gets a lot of extremely detailed metadata about the computational environment, which is what allows us to rebuild it automatically for people later. The RPC format itself is very generalizable, it's a fancy zip, you can unzip it and look at the contents on your computer, you can use it with many, many different types of software. So it's a format that can be widely used. And we've tried to future proof it such that to be able to set up that environment on someone else's machine, we use a plug in model so as long as there are virtual machines, containers, Eaasi or Linux in the future, you will always be able to re execute the contents of a reprozip bundle. And this is basically the workflow right so you would trace the there's two steps in the packing you trace and pack you get this RPZ bundle. You can unpack it in the web with repro server or again, you can use one of these unpackers there's Docker vagrants, we have a singularity unpacker coming, you can use it with a few other tools. So it's a it's a way to have it. Encourage sustainability at the level of a researcher. And basically how it works is like you work normally you run reprozip trace you run reprozip pack, you get the RPZ, bing, bang, boom, that's it. And it can basically pack anything that runs on a computer including things like MPI, experiments, client server applications, graphical software, interactive tools, if you can run it, we can probably pack it. And that includes proprietary software. It's just rerunning proprietary software on someone else's computer gets a little dicey.
 
So again, this is how it works. It not only gets, like I said, Does input files and output files and parameters. But let's say you're running like three scripts in a row represent will know the order to run the scripts it takes it understands the provenance and steps, as well as some of those lower level computational environment variables, like what specific version of Linux did you use? What is the specific again, like dependencies, it's written into a metadata file. And then all packed together in this distributable RPZ, which again, can be sent to anybody else to be used on any other computer that they want. And there Here are some current use cases of represent. So it's been used as widely to facilitate peer review to share reproducible research. It's actually been integrated as a part of other reproducibility tools to provide the reproducibility and sustainability aspect. It's been integrated in different computational science tools, and also as a part of existing digital preservation infrastructure. So I mentioned Eaasi before, if we pack something with reprozip it that runs on a really old, a really old operating system that we might not be able to reproduce you know in a In a VM or in Docker, if we can, we can use something like Eaasi to be able to give that a sandbox environment of an old and older runtime to be able to get at the unpacked contents. So retros is one way, again, encouraging sustainability efforts in our communities. I have 39 seconds according to my watch, so maybe I'll skip the demo. But I do have some demo videos, and I've included them in the, in the links as well. So just overall, all research is, is founded on different computational environments, we need to understand those for your producibility. And we need to preserve those for reproducibility the future. So that scholarship can build on itself, like we like to say that it does represent can help with software sustainability through a creation of these preservation ready bundles that are inclusive of software and also contains some of that in depth technical metadata that makes sustainability a little bit easier, although it does pack the mess. So I think that's a nice callback to Collins talk. If you're you have a big tangled web represent we'll preserve the big tangled web. So there are some still needs for interventions for stewardship of materials, you know, before it gets the archive before it's packed. So that's reprozip. I'm happy to take questions asynchronously or if resume and love for you to try it out and feel free to hit me up anywhere. Thank you so much.

