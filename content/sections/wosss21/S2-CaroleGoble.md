# FAIR Computational Workflows
**by Carole Goble**  

## From session 2 - [Open Science & applying the FAIR principles to software](/wosss21/agenda#session-2)  

The definitive write-up of this talk appears in the [WoSSS21 report](https://wosss.org/#reports).

### Resources
A [video](https://www.youtube.com/watch?v=l_sQoChDTHU&list=PLXAvKzjdTsrxFqbjWtxHjfJc0RN6jMwZg&index=11) and [slides](https://docs.google.com/presentation/d/15DgUfb-CQ65LYuBu_vAAuNqPyFPD7kNZ/edit?usp=sharing&ouid=111252485485949924310&rtpof=true&sd=true) are available.

### Transcript

#### keywords

workflows, fair, workflow management, data, software, metadata, building, develop, communities, commons, work, requires, canonical, systems, execution, codes, repositories, talking, registries, activities

#### Text

Okay, so thank you very much for inviting me, I'm going to briefly talk about fair computational workflows. And I'm doing this from the setting of very large research infrastructures at the European level. I'm also a member of the software Sustainability Institute in the UK. So computational workflows, if I can move my slides on,

which doesn't seem to be working. Yeah, there we go. So computational workflows are particular kinds of software. As we're talking about software here. I working in the ER skull Life Project, which is the A European Open Science Cloud life, large cluster, which brings together European Research infrastructures dedicated to building a collaborative space for digital biology. And what we're doing there is building a fair data and work for commons, not unlike what Tom was talking about this earlier about the bio Commons, and other Commons in Australia. And then we have an extensive use of computational workflows for preparing, analysing, and increasingly sharing a large volumes of data, we have very different kinds of workflow management systems, they all are effectively building multi step pipelines and multi step processes to coordinate and execute multiple codes, codes that they may not have actually developed themselves internally. So external codes. And those workflow systems are handling data and processing dependencies and doing other kinds of heavy lifting. So we're talking about typically a data pipelines. So we have an interesting thing here with computation and workflows, that there are special kinds of software. But they're also a precise description of a process. So I wanted to highlight from the point of view of fair, two major aspects. The first is that workflow management systems have the sense of abstraction. So they have a workflow specification, which is a description with parameters and inputs and guidance. And there, we're talking about fair transparency, and meta descriptions. And there, we can consider these almost to be like FAIR data, because they're descriptive artefacts. On the other hand, we have software, we have the workflow management systems themselves, as well as the tools and the infrastructure that the individual codes that they're orchestrating and chaining together. And that's very much around the fair software, which is also related to more reproducibility of being able to run those pipelines and reuse those pipelines. And then we're thinking not so much about method preservation, but as much as software preservation. And then combined with those two, we have the associated objects around those workflows, logs of their execution, example data, test data, and services associated with them in order to be able to check whether these workflows are fair. The second abstraction that we have in workflow management systems is the notion of extreme modularization and composability, that we expect to take various different components in different languages from different third parties, and be able to put them together and put them and then to recombine them and put them to yet more places. So we're really looking at workflows are compositions of components, including other workflows that can be broken down, versioned, recycled and so on. So this requires fair to apply it the different levels of abstraction at the description level and the software level, and for the different components that make up the workflows. We have multiple workflow system in our landscape, which typically are used in an intertwined kind of way. So people use a workflow management system, which is a dedicated infrastructure that does that neat separation of concerns with respect to modularization, but also abstraction, and and execution. But these are typically used also with interactive notebooks, and scripting environments, which perhaps are less clean in their regard. But still, we're still doing multiple steps. There are about 300 different workflow management systems currently available. And with that, we also have repositories, registries, general repositories and archives that those dedicated registers and repositories with respect to workflows, work with and dedicated workflow services. So we've got quite a range of, of activities, and all of this has to be respected in our Collaboratory. So we have to honour diversity and legacy. So with respect to workflows, we have that workflows are both method objects, software objects, so they inherit properties of workflows as bare digital objects from the FAIR data principles, their software objects don't inherit from parties from the software principles, but they also are instruments of data for verification themselves, because they typically are dealing with data flow. And there, they should be supporting data.

FAIR data. And if we if we go back to the FAIR data principles, we'll remember that they emphasise machine actionable metadata to aid automated processing. That was the point of the fair principles. So I'll quickly in my last few minutes, go through a whole bunch of activities that we're doing in the Eos glyph project in order to start to build an environment for fair workflows. The first is to support find stability, and indexing and access. And then we're building registries, we build the workflow hub, in fact, which is a registry of workflows. And I give you a glimpse here, which links into and leverages different workflow management systems, their different deployments that they'd be running and repositories. And we have to deal with a whole bunch of access and reuse requirements, for example, licencing, provenance of the workflow, and also the workflow executions, building libraries of them, lifecycle support around versioning, and Registration Access in different ways. And we're leveraging things like the GA for G H tool registry service for access. An important aspect here is we we are developing machine actionable metadata for workflows because machine actionable metadata is the key of fair. And that means we have to have a standard framework. And we have to be able to auto harvest fair metadata from all of these different workflow management systems we're working with. And that means it is essential to onboard these workflow management systems in our efforts towards fair. So are a workflow metadata frameworks we've been developing cover aspects to do with canonical descriptions, interoperability,

common metadata about what the workflows are about so that they can be registered and exchanged, and also ways of packaging, all of the different disparate components that I highlighted earlier, around workflows into something called a workflow, our crate, which we do joint with Australian colleagues, actually, which also is an opportunity for us to package also the logging and data lineage results of running a particular workflow. So this covers the many aspects of fair work, fair software, and workflows, in order to be able to make sure that metadata are accessible even when the workflow or its software has disappeared. And we use all of this framework in order to be able to move workflows around our system of services and registries. So even if our workflow hub disappears, all the metadata associated with the workflow can be deposited into the nodo, the software might not exist, but the description still well, from the point of view of interoperability, workflows have two aspects. The first is interoperability that deals with all the workflow management systems. So we can describe the workflows independently of their many different languages that they use. And that enables platform independent pipeline exchange. And there are two major languages that have been developed in that area, we particularly use a common workflow language. And the second is workflow composability. There's modularization concept where the independent components will operate through API's and metadata standards. So that requires programmatic, programmatic access to the metadata of those particular tools, but also making workflow ready tools, and being able to create canonical and recyclable workflow blocks. And there are a number of communities that have been developing those kinds of blocks that they can be put together and re re composed. And like like Lego effectively. And that requires quite a lot of work on clean interfaces, clean use of identifiers, and designing from the outset for a fair data and fair workflow, reuse in those in those workflows. And then finally, we have reusability and usability. And there, of course, all the metadata that we're developing enables reusable, so I'm taking this from the fair for Rs guidelines that were usable means that it can be understood, modified, built upon or incorporated into other workflows. But usable is can it still be executed. And then we have to develop at the fair software level at the workflow management systems in the code. They run frameworks around packaging using containers, testing and monitoring, and also execution standards and API's such as the GA for GH standard for being able to run workflows when you find them. A brief comment that two workflows are actually fine For FAIR data, so that means that we also have to review whether the workflows actually produce FAIR data. So are they licencing data outputs? Do they use community data formats? What usage restrictions do they require? Do they handle identifiers correctly, this is absolutely critical for the detailed provenance of the data that goes to those workflows. And there, again, we're advocating good design for FAIR data, and reuse and the development of these canonical workflows and libraries of validated and curated workflows by communities, as well as best practice and golden examples of workflows, which we go through a reviewing process. And that really requires a lot of training and stewardship and sustainability activities. So to wrap up, a key thing that we talked about yesterday is the importance of collective action and communities. And this mantra, that fair takes a village and I've made reference to a very key paper, I think that speaks about this. And here workflows, as opposed to all of software, we have actually a subset of community, which we can point to, and and work with. So there are communities of workflow management system, Platform Developers, there are communities working on software, fair principles, we have communities of workflow developers that are building these well curated and canonical workflows that we can address directly in order to be able to improve their practices, as well as those working in building the standards and communities for building sustainability and policy around fair workflows. In a fair Data Commons, our fair Commons, and I'm particularly representing here, as I say, the European world.

Finally, as software developers have many of you as software developers, we have many different challenges still to deal with, with the fair workflows, we still actually have to define the principles. And particularly considering this complex lifecycle of specification of execution and data products, and also metrics around the fairness of workflows. So we need to develop some rules and practices and recommendations for that communities that I've just highlighted. But we also need folks that develop the code that are not expected they, when they develop those codes, they perhaps did not expect them to be incorporated in workflows, but we need code to become workflow friendly. And that means clean interfaces, and avoiding uses restrictions and so on, as well as fair workflow making. So are, can we automate the fairness in workflows and check the the way that those workflows have been developed, so that they actually adhere to fair principles, not just for the workflow, but also for the data that flows through them. And we should not forget the fair workflow user as well, that we want to encourage people to use well documented, fair, enabling and fair workflows and to credit the makers of them because this is a non trivial and expensive activity. And I really liked the term that was used before on on ecosystem citizenship. And that's my, the end of my very fast talk.

