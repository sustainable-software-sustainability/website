# FAIR Computational Workflows
**by Carole Goble**  

## From session 2 - [Open Science & applying the FAIR principles to software](/wosss21/agenda#session-2)  
The FAIR principles (Findable, Accessible, Interoperable, Reusable) [1] have laid a foundation for sharing and publishing digital assets, starting with data and now extending to all digital objects including software [2].  

Computational workflows are a special kind of software for handling multi-step, multi-code data pipelines, analysis and simulations. Their use has accelerated in the past few years driven by the need for repetitive and scalable data processing, access to and exchange of processing know-how, and the desire for more reproducible (or at least transparent) and quality assured processing methods [3]. COVID-19 pandemic has highlighted the value of workflows [4].  
Computational workflows encode the methods by which the scientific process is conducted and via which data are created, by capturing precise descriptions of the multiple execution steps and data dependencies needed.  Workflow Management Systems and execution platforms handle the definition and set-up of the multi-step specification and the heavy lifting of dependency management, code execution, data and control flow, reporting and monitoring. Over 290 workflow systems are [currently available](https://s.apache.org/existing-workflow-systems), although a much smaller number are widely adopted [5].  

As first class, publishable research objects, it seems natural to apply FAIR principles to workflows [6].  The FAIR data principles themselves originate from a desire to support automated data processing, by emphasizing machine accessibility of data and metadata. As workflows have a dual role as software and explicit method description, their FAIR properties draw from both data and software principles for descriptive metadata, software metrics, and versioning. However, workflows create unique challenges such as representing a complex lifecycle from specification to execution via a workflow system, through to the data created at the completion of the workflow. As workflows are chiefly concerned with the processing and creation of data they have an important role to play in ensuring and supporting data FAIRification.  

The work on defining and improving the FAIRness of workflows has already started. A whole ecosystem of tools, guidelines and best practices are under development to reduce the time needed to adapt, reuse and extend existing scientific workflows.  For example, a fundamental tenet of FAIR is the universal availability of machine processable metadata. The [European EOSC-Life Cluster](https://www.eosc-life.eu/) has developed a metadata framework for FAIR workflows based on [schema.org](https://bioschemas.org/profiles/ComputationalWorkflow/1.0-RELEASE/), [RO-Crate](https://www.researchobject.org/ro-crate/) [7] and [Common Workflow Language](https://www.commonwl.org/) (CWL) [8], and uses the [GA4GH TRS API](https://ga4gh.github.io/tool-registry-service-schemas/) for a standardised communication protocol to support Accessibility. It has developed and runs the [WorkflowHub](https://workflowhub.eu/) registry which uses both the framework and the protocol to support workflow Findability. EOSC-Life have made great efforts to on-board community workflow platforms such as [Galaxy](https://galaxyproject.org/), [snakemake](https://snakemake.readthedocs.io/en/stable/), [nextflow](https://www.nextflow.io/) and CWL to carry and use FAIR metadata for discovery and reuse.  As FAIR software needs to be usable and not just reusable, EOSC-Life has also developed services for, e.g. workflow testing ([LifeMonitor](https://crs4.github.io/life_monitor/)), execution and benchmarking.

The Interoperability principle is the hardest to unpack for both data and software. For workflows, interoperability follows two threads: (i) supporting workflow system interoperability through workflow descriptions independent of the underlying system (e.g. CWL and [WDL](https://openwdl.org/)) and (ii) workflow component composability. Workflows are ideally composed of modular building blocks and these and the workflows themselves are expected to be reused, refactored, recycled and remixed. Thus FAIR applies "all the way down": at the specification and execution level, and for the whole workflow and each of its components. Composability also relates to reuse – that is, adapting [2], a workflow or its component “can be understood, modified, built upon or incorporated into other workflow”.  Reuse challenges also include being able to capture and then move workflow components, dependencies, and application environments in such a way as not to affect the resulting execution of the workflow. Interoperability and Reusability present important obligations on software developers to ensure that tools and datasets are workflow ready data with clean I/O programmatic interfaces, no usage restrictions, use of community data standards, and that they are simple to install and designed for portability. Workflow developers can be both data-FAIR,  by using and making identifiers, licensing data outputs, tracking data provenance and so on, and workflow-FAIR by managing versions, providing test data, and sharing libraries of composable and reusable workflow “blocks” [9]. Communities are working on reviewing, validating and certifying canonical workflows.  

While there are emerging tools for addressing different aspects of FAIR workflows, many challenges remain for describing, annotating, and exposing scientific workflows so that they can be found, understood and reused by other scientists. Further work is required to understand use cases for reuse and enable reuse in the same or different environments. The FAIR principles for workflows need to be community-agreed before metrics can be considered to determine whether a workflow is FAIR, whether a workflow repository or registry is FAIR, and whether it is possible to automatically review whether a workflow’s dataflow is FAIR. Community activism, perhaps led by the platforms and registries coming together in a community group like [WorkflowsRI](https://workflowsri.org/), is needed to define principles, policies and best practices for FAIR workflows and to standardize metadata representation and collection processes.

[1] M.D. Wilkinson, M Dumontier et al, The FAIR Guiding Principles for scientific data management and stewardship, Scientific Data 3, (2016), https://doi.org/10.1038/sdata.2016.18  
[2] D.S. Katz, M Gruenpeter, T Honeyman Taking a fresh look at FAIR for research software PATTERNS 2(2) (2021) https://doi.org/10.1016/j.patter.2021.100222  
[3] T Reiter, P.T Brooks, L Irber, S.E.K Joslin, C.M Reid, C Scott, C.T Brown, N.T Pierce-Ward, Streamlining data-intensive biology with workflow systems, GigaScience, 10(1) 2021, giaa140, https://doi.org/10.1093/gigascience/giaa140  
[4] W Maier, S Bray et al Freely accessible ready to use global infrastructure for SARS-CoV-2 monitoring bioRxiv (2021) doi: https://doi.org/10.1101/2021.03.25.437046  
[5] L. Wratten, A. Wilm, J Göke Reproducible, scalable, and shareable analysis pipelines with bioinformatics workflow managers. Nat Methods (2021). https://doi.org/10.1038/s41592-021-01254-9  
[6] C Goble, S Cohen-Boulakia, S Soiland-Reyes, D Garijo, Y Gil, M.R. Crusoe, K Peters, D Schober FAIR Computational Workflows Data Intelligence 2020 2:1-2, 108-121 https://doi.org/10.1162/dint_a_00033.  
[7] S Soiland-Reyes, P Sefton, et al Packaging research artefacts with RO-Crate, [arXiv:2108.06503v1](https://arxiv.org/abs/2108.06503v1)  
[8] M Crusoe et al Methods Included: Standardizing Computational Reuse and Portability with the Common Workflow Language, CACM (2021), https://doi.org/10.1145/3486897  
[9] P Andrio, A Hospital, J Conejero et al. BioExcel Building Blocks, a software library for interoperable biomolecular simulation workflows. Sci Data 6, 169 (2019). https://doi.org/10.1038/s41597-019-0177-4
